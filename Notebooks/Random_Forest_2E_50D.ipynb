{"cells":[{"cell_type":"markdown","id":"a6a69c9e","metadata":{},"source":["## Big Data Project - Model2: Random Forest\n","### This file contain code and result of Random Forest trained on review of books.jsonl file with 2 executor on 50 percent of actual dataset"]},{"cell_type":"code","execution_count":1,"id":"07440a00","metadata":{},"outputs":[],"source":["import pandas as pd\n","import pyspark.sql.functions as F\n","from pyspark.sql.functions import col, count, when"]},{"cell_type":"markdown","id":"648b09bb","metadata":{},"source":["### --Creating and building spark session with 2 maxExecutors with 50 partition"]},{"cell_type":"code","execution_count":2,"id":"1cff0e21","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 15:00:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Creating a SparkSession\n","spark = (\n","  SparkSession.builder\n","    .appName(\"AmazonReviewsUsingBERT\")\n","    .master(\"yarn\")\n","    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n","    .config(\"spark.shuffle.service.enabled\",   \"true\")\n","    .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n","    .config(\"spark.dynamicAllocation.maxExecutors\",\"2\")\n","    .config(\"spark.sql.shuffle.partitions\",       \"50\")\n","    .getOrCreate()\n",")"]},{"cell_type":"markdown","id":"e84a2d42","metadata":{},"source":["### --Data Loading and preprocessing\n","#### I have turned my data as public so you should be able to run the below code, if there is problem while running then please let me know."]},{"cell_type":"code","execution_count":3,"id":"5c35b2c5","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Loadig the JSONL file into a Spark DataFrame\n","df = spark.read.json(\"gs://bigdataprojectdata/notebooks/jupyter/Books.jsonl\")\n"]},{"cell_type":"code","execution_count":4,"id":"81ffd649","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 1:======================================================>(148 + 2) / 150]\r"]},{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+------+----+---------+-----+-------+-----------------+\n","|asin|helpful_vote|images|parent_asin|rating|text|timestamp|title|user_id|verified_purchase|\n","+----+------------+------+-----------+------+----+---------+-----+-------+-----------------+\n","|   0|           0|     0|          0|     0|   0|        0|    0|      0|                0|\n","+----+------------+------+-----------+------+----+---------+-----+-------+-----------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["#HANDLING MISSING VALUES\n","df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"]},{"cell_type":"code","execution_count":5,"id":"5dd3123d","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import when\n","df = df.withColumn(\n","    \"sentiment\", \n","    when(df[\"rating\"] >= 4, \"positive\")\n","    .when(df[\"rating\"] <= 3, \"negative\")\n",")"]},{"cell_type":"code","execution_count":6,"id":"ffe1e171","metadata":{},"outputs":[],"source":["df = df.dropDuplicates([\"title\", \"user_id\"])"]},{"cell_type":"code","execution_count":7,"id":"34ede670","metadata":{},"outputs":[],"source":["df = df.select(\"text\",\"rating\", \"sentiment\")"]},{"cell_type":"code","execution_count":8,"id":"0b32acab","metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[text: string, rating: double, sentiment: string]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["#checking if reviews \"text\" is a proper review i.e. contains more than 3 words. Dropping all the rows which has text containg less that 4 words\n","df = (df\n","                         .withColumn(\"word_count\", F.size(F.split(F.col(\"text\"),r\"\\s+\")))\n","                         .filter(F.col(\"word_count\")>=4)\n","                         .drop(\"word_count\")\n","    )\n","df"]},{"cell_type":"code","execution_count":9,"id":"9e754ed9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 6:=======================================================> (49 + 1) / 50]\r"]},{"name":"stdout","output_type":"stream","text":["+---------+------------+\n","|sentiment|review_count|\n","+---------+------------+\n","| negative|     4144153|\n","| positive|    21092378|\n","+---------+------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["#checkign the counts of sentiment rows if they are balanced or not\n","counts_sentiments = (\n","    df\n","        .groupBy(\"sentiment\")\n","        .agg(F.count(\"*\").alias(\"review_count\"))\n","        .orderBy(\"sentiment\")\n",")\n","counts_sentiments.show()"]},{"cell_type":"code","execution_count":10,"id":"df64f513","metadata":{},"outputs":[],"source":["fractions = {\"positive\": 0.4, \"negative\": 1.0}\n","\n","df = df.sampleBy(\"sentiment\", fractions, seed=42)\n","\n"]},{"cell_type":"code","execution_count":11,"id":"7fbba7bd","metadata":{},"outputs":[],"source":["# Sampling only 50% of data \n","df = df.sample(withReplacement=False, fraction=0.50, seed=42)"]},{"cell_type":"code","execution_count":12,"id":"2df65579","metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline\n","from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n","from pyspark.ml.classification import RandomForestClassifier"]},{"cell_type":"code","execution_count":13,"id":"e5ed8477","metadata":{},"outputs":[],"source":["# Text preprocessing\n","tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n","remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n","hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n","idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"]},{"cell_type":"code","execution_count":14,"id":"640d645b","metadata":{},"outputs":[],"source":["\n","# Label encoding \n","indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")"]},{"cell_type":"code","execution_count":15,"id":"52a28e5d","metadata":{},"outputs":[],"source":["# Random Forest Classifier\n","rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)"]},{"cell_type":"code","execution_count":16,"id":"8d330efc","metadata":{},"outputs":[],"source":["# Combine all into a pipeline\n","pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, indexer, rf])"]},{"cell_type":"code","execution_count":17,"id":"41d27518","metadata":{},"outputs":[],"source":["# Split data\n","train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)"]},{"cell_type":"code","execution_count":18,"id":"6099dc36","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 15:39:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_16 !\n","25/05/05 15:39:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_8 !\n","25/05/05 15:39:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_12 !\n","25/05/05 15:39:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_0 !\n","25/05/05 15:39:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_20 !\n","25/05/05 15:39:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_28 !\n","25/05/05 15:39:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_4 !\n","25/05/05 15:39:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_24 !\n","25/05/05 15:39:25 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000002 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 137. Diagnostics: [2025-05-05 15:39:25.002]Container killed on request. Exit code is 137\n","[2025-05-05 15:39:25.009]Container exited with a non-zero exit code 137. \n","[2025-05-05 15:39:25.011]Killed by external signal\n",".\n","25/05/05 15:39:25 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1746457157008_0001_01_000002 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 137. Diagnostics: [2025-05-05 15:39:25.002]Container killed on request. Exit code is 137\n","[2025-05-05 15:39:25.009]Container exited with a non-zero exit code 137. \n","[2025-05-05 15:39:25.011]Killed by external signal\n",".\n","25/05/05 15:39:25 ERROR YarnScheduler: Lost executor 2 on cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000002 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 137. Diagnostics: [2025-05-05 15:39:25.002]Container killed on request. Exit code is 137\n","[2025-05-05 15:39:25.009]Container exited with a non-zero exit code 137. \n","[2025-05-05 15:39:25.011]Killed by external signal\n",".\n","25/05/05 15:39:25 WARN TaskSetManager: Lost task 28.0 in stage 30.0 (TID 1388) (cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000002 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 137. Diagnostics: [2025-05-05 15:39:25.002]Container killed on request. Exit code is 137\n","[2025-05-05 15:39:25.009]Container exited with a non-zero exit code 137. \n","[2025-05-05 15:39:25.011]Killed by external signal\n",".\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_31 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_36 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_7 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_23 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_2 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_33 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_19 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_11 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_15 !\n","25/05/05 15:41:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_27 !\n","25/05/05 15:41:36 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000001 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:41:35.712]Container killed on request. Exit code is 143\n","[2025-05-05 15:41:35.715]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:41:35.716]Killed by external signal\n",".\n","25/05/05 15:41:36 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1746457157008_0001_01_000001 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:41:35.712]Container killed on request. Exit code is 143\n","[2025-05-05 15:41:35.715]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:41:35.716]Killed by external signal\n",".\n","25/05/05 15:41:36 ERROR YarnScheduler: Lost executor 1 on cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000001 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:41:35.712]Container killed on request. Exit code is 143\n","[2025-05-05 15:41:35.715]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:41:35.716]Killed by external signal\n",".\n","25/05/05 15:41:36 WARN TaskSetManager: Lost task 40.0 in stage 30.0 (TID 1401) (cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000001 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:41:35.712]Container killed on request. Exit code is 143\n","[2025-05-05 15:41:35.715]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:41:35.716]Killed by external signal\n",".\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_5 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_35 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_1 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_39 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_9 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_17 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_28 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_25 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_13 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_21 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_29 !\n","25/05/05 15:43:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_43 !\n","25/05/05 15:43:42 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000004 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:43:42.000]Container killed on request. Exit code is 143\n","[2025-05-05 15:43:42.000]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:43:42.000]Killed by external signal\n",".\n","25/05/05 15:43:42 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_1746457157008_0001_01_000004 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:43:42.000]Container killed on request. Exit code is 143\n","[2025-05-05 15:43:42.000]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:43:42.000]Killed by external signal\n",".\n","25/05/05 15:43:42 ERROR YarnScheduler: Lost executor 3 on cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000004 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:43:42.000]Container killed on request. Exit code is 143\n","[2025-05-05 15:43:42.000]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:43:42.000]Killed by external signal\n",".\n","25/05/05 15:43:42 WARN TaskSetManager: Lost task 47.0 in stage 30.0 (TID 1409) (cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000004 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:43:42.000]Container killed on request. Exit code is 143\n","[2025-05-05 15:43:42.000]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:43:42.000]Killed by external signal\n",".\n"]},{"name":"stderr","output_type":"stream","text":["25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_49 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_6 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_41 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_26 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_34 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_18 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_10 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_22 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_14 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_30 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_37 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_3 !\n","25/05/05 15:45:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_45 !\n","25/05/05 15:45:26 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000005 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:45:25.755]Container killed on request. Exit code is 143\n","[2025-05-05 15:45:25.755]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:45:25.756]Killed by external signal\n",".\n","25/05/05 15:45:26 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1746457157008_0001_01_000005 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:45:25.755]Container killed on request. Exit code is 143\n","[2025-05-05 15:45:25.755]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:45:25.756]Killed by external signal\n",".\n","25/05/05 15:45:26 ERROR YarnScheduler: Lost executor 4 on cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000005 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:45:25.755]Container killed on request. Exit code is 143\n","[2025-05-05 15:45:25.755]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:45:25.756]Killed by external signal\n",".\n","25/05/05 15:45:26 WARN TaskSetManager: Lost task 14.0 in stage 33.0 (TID 1475) (cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000005 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:45:25.755]Container killed on request. Exit code is 143\n","[2025-05-05 15:45:25.755]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:45:25.756]Killed by external signal\n",".\n","25/05/05 15:47:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_0 !\n","25/05/05 15:47:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_4 !\n","25/05/05 15:47:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_47 !\n","25/05/05 15:47:04 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000008 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:47:03.951]Container killed on request. Exit code is 143\n","[2025-05-05 15:47:03.952]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:47:03.952]Killed by external signal\n",".\n","25/05/05 15:47:04 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 7 for reason Container from a bad node: container_1746457157008_0001_01_000008 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:47:03.951]Container killed on request. Exit code is 143\n","[2025-05-05 15:47:03.952]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:47:03.952]Killed by external signal\n",".\n","25/05/05 15:47:04 ERROR YarnScheduler: Lost executor 7 on cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000008 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:47:03.951]Container killed on request. Exit code is 143\n","[2025-05-05 15:47:03.952]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:47:03.952]Killed by external signal\n",".\n","25/05/05 15:47:04 WARN TaskSetManager: Lost task 9.0 in stage 33.0 (TID 1483) (cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000008 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:47:03.951]Container killed on request. Exit code is 143\n","[2025-05-05 15:47:03.952]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:47:03.952]Killed by external signal\n",".\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_5 !\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_19 !\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_11 !\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_42 !\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_22 !\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_15 !\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_1 !\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_46 !\n","25/05/05 15:52:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_26 !\n","25/05/05 15:52:45 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000007 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:52:45.136]Container killed on request. Exit code is 143\n","[2025-05-05 15:52:45.136]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:52:45.137]Killed by external signal\n",".\n","25/05/05 15:52:45 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1746457157008_0001_01_000007 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:52:45.136]Container killed on request. Exit code is 143\n","[2025-05-05 15:52:45.136]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:52:45.137]Killed by external signal\n",".\n","25/05/05 15:52:45 ERROR YarnScheduler: Lost executor 6 on cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000007 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:52:45.136]Container killed on request. Exit code is 143\n","[2025-05-05 15:52:45.136]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:52:45.137]Killed by external signal\n",".\n","25/05/05 15:52:45 WARN TaskSetManager: Lost task 30.0 in stage 33.0 (TID 1505) (cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000007 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 15:52:45.136]Container killed on request. Exit code is 143\n","[2025-05-05 15:52:45.136]Container exited with a non-zero exit code 143. \n","[2025-05-05 15:52:45.137]Killed by external signal\n",".\n"]},{"name":"stderr","output_type":"stream","text":["25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_5 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_32 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_16 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_44 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_36 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_7 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_2 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_33 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_12 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_20 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_38 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_42 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_48 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_40 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_28 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_15 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_43 !\n","25/05/05 16:00:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_24 !\n","25/05/05 16:00:38 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000006 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 137. Diagnostics: [2025-05-05 16:00:37.955]Container killed on request. Exit code is 137\n","[2025-05-05 16:00:37.955]Container exited with a non-zero exit code 137. \n","[2025-05-05 16:00:37.955]Killed by external signal\n",".\n","25/05/05 16:00:38 ERROR YarnScheduler: Lost executor 5 on cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000006 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 137. Diagnostics: [2025-05-05 16:00:37.955]Container killed on request. Exit code is 137\n","[2025-05-05 16:00:37.955]Container exited with a non-zero exit code 137. \n","[2025-05-05 16:00:37.955]Killed by external signal\n",".\n","25/05/05 16:00:38 WARN TaskSetManager: Lost task 16.0 in stage 39.0 (TID 1685) (cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000006 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 137. Diagnostics: [2025-05-05 16:00:37.955]Container killed on request. Exit code is 137\n","[2025-05-05 16:00:37.955]Container exited with a non-zero exit code 137. \n","[2025-05-05 16:00:37.955]Killed by external signal\n",".\n","25/05/05 16:00:38 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1746457157008_0001_01_000006 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 137. Diagnostics: [2025-05-05 16:00:37.955]Container killed on request. Exit code is 137\n","[2025-05-05 16:00:37.955]Container exited with a non-zero exit code 137. \n","[2025-05-05 16:00:37.955]Killed by external signal\n",".\n","25/05/05 16:04:43 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_49 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_35 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_1 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_31 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_44 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_23 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_39 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_26 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_9 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_17 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_10 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_38 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_28 !\n","25/05/05 16:06:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_89_27 !\n","25/05/05 16:06:14 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000010 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 16:06:14.409]Container killed on request. Exit code is 143\n","[2025-05-05 16:06:14.409]Container exited with a non-zero exit code 143. \n","[2025-05-05 16:06:14.410]Killed by external signal\n",".\n","25/05/05 16:06:14 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 9 for reason Container from a bad node: container_1746457157008_0001_01_000010 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 16:06:14.409]Container killed on request. Exit code is 143\n","[2025-05-05 16:06:14.409]Container exited with a non-zero exit code 143. \n","[2025-05-05 16:06:14.410]Killed by external signal\n",".\n","25/05/05 16:06:14 ERROR YarnScheduler: Lost executor 9 on cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000010 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 16:06:14.409]Container killed on request. Exit code is 143\n","[2025-05-05 16:06:14.409]Container exited with a non-zero exit code 143. \n","[2025-05-05 16:06:14.410]Killed by external signal\n",".\n","25/05/05 16:06:14 WARN TaskSetManager: Lost task 38.0 in stage 42.0 (TID 1803) (cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000010 on host: cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 16:06:14.409]Container killed on request. Exit code is 143\n","[2025-05-05 16:06:14.409]Container exited with a non-zero exit code 143. \n","[2025-05-05 16:06:14.410]Killed by external signal\n",".\n","[Stage 45:=====================================================>(149 + 1) / 150]\r"]},{"name":"stdout","output_type":"stream","text":["Training Time: 3859.86 seconds\n"]}],"source":["import time\n","start = time.time()\n","model = pipeline.fit(train_df)\n","predictions = model.transform(test_df)\n","end = time.time()\n","print(f\"Training Time: {end - start:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":19,"id":"a9a138e9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 48:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+---------+----------+\n","|                text|sentiment|prediction|\n","+--------------------+---------+----------+\n","|\" '...There will ...| negative|       0.0|\n","|\"\"\" Lies<br />by ...| negative|       0.0|\n","|\"...In the year 2...| positive|       0.0|\n","|\"...you cannot se...| positive|       0.0|\n","|\"A Dragons Tale\" ...| positive|       0.0|\n","|\"A Navy Maverick ...| negative|       0.0|\n","|\"A man lay on the...| positive|       0.0|\n","|\"All that Glitter...| positive|       0.0|\n","|\"Athena\" -- starr...| negative|       0.0|\n","|\"BIRDS OF BRITAIN...| positive|       0.0|\n","+--------------------+---------+----------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["predictions.select(\"text\", \"sentiment\", \"prediction\").show(10)"]},{"cell_type":"code","execution_count":20,"id":"d4319de3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 16:15:40 WARN YarnAllocator: Container from a bad node: container_1746457157008_0001_01_000009 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 16:15:40.161]Container killed on request. Exit code is 143\n","[2025-05-05 16:15:40.163]Container exited with a non-zero exit code 143. \n","[2025-05-05 16:15:40.163]Killed by external signal\n",".\n","25/05/05 16:15:40 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 8 for reason Container from a bad node: container_1746457157008_0001_01_000009 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 16:15:40.161]Container killed on request. Exit code is 143\n","[2025-05-05 16:15:40.163]Container exited with a non-zero exit code 143. \n","[2025-05-05 16:15:40.163]Killed by external signal\n",".\n","25/05/05 16:15:40 ERROR YarnScheduler: Lost executor 8 on cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal: Container from a bad node: container_1746457157008_0001_01_000009 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 16:15:40.161]Container killed on request. Exit code is 143\n","[2025-05-05 16:15:40.163]Container exited with a non-zero exit code 143. \n","[2025-05-05 16:15:40.163]Killed by external signal\n",".\n","25/05/05 16:15:40 WARN TaskSetManager: Lost task 0.0 in stage 51.0 (TID 2469) (cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746457157008_0001_01_000009 on host: cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal. Exit status: 143. Diagnostics: [2025-05-05 16:15:40.161]Container killed on request. Exit code is 143\n","[2025-05-05 16:15:40.163]Container exited with a non-zero exit code 143. \n","[2025-05-05 16:15:40.163]Killed by external signal\n",".\n","[Stage 51:======================================================> (49 + 1) / 50]\r"]},{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.6702\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","print(f\"Test Accuracy: {accuracy:.4f}\")"]},{"cell_type":"code","execution_count":21,"id":"b49cc36a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","bce = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n","                                    labelCol=\"label\",\n","                                    metricName=\"areaUnderROC\")\n","roc_auc = bce.evaluate(predictions)"]},{"cell_type":"code","execution_count":22,"id":"fb74d7f7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n"]},{"cell_type":"code","execution_count":23,"id":"3ef05508","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test ROC AUC = 0.7638\n","Test Accuracy = 0.6702\n","Test F1 Score = 0.5383\n"]}],"source":["print(f\"Test ROC AUC = {roc_auc:.4f}\")\n","print(f\"Test Accuracy = {accuracy:.4f}\")\n","print(f\"Test F1 Score = {f1:.4f}\")"]},{"cell_type":"code","execution_count":24,"id":"1ec02e12","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Active Executors: Set(cluster-c2aa-m.northamerica-south1-b.c.academic-timing-458516-v2.internal:33325, cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal:46435, cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal:42817, cluster-c2aa-w-0.northamerica-south1-b.c.academic-timing-458516-v2.internal:42183, cluster-c2aa-w-1.northamerica-south1-b.c.academic-timing-458516-v2.internal:46815)\n"]}],"source":["from pyspark import SparkContext\n","\n","sc = spark.sparkContext\n","executors = sc._jsc.sc().getExecutorMemoryStatus().keySet()\n","print(f\"Active Executors: {executors}\")"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}